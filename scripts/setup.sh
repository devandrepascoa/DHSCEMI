#!/bin/bash

set -e

echo "ðŸš€ Setting up LLM Benchmarking System..."

if ! command -v docker &> /dev/null; then
    echo "âŒ Docker is not installed. Please install Docker first."
    exit 1
fi

# Check NVIDIA Docker support for CUDA
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… NVIDIA GPU detected"
    if docker info | grep -q nvidia; then
        echo "âœ… NVIDIA Docker runtime available"
    else
        echo "âš ï¸  NVIDIA Docker runtime not found. CUDA benchmarking may not work."
    fi
else
    echo "âš ï¸  NVIDIA GPU not detected. CUDA benchmarking will be skipped."
fi

echo "ðŸ³ Pulling prebuilt Docker images..."

echo "  Pulling CPU image..."
docker pull ghcr.io/ggml-org/llama.cpp:full

if command -v nvidia-smi &> /dev/null; then
    echo "  Pulling CUDA image..."
    docker pull ghcr.io/ggml-org/llama.cpp:full-cuda
else
    echo "  Skipping CUDA image (no NVIDIA GPU detected)"
fi

echo "ðŸ“¦ Installing Uv dependencies..."
uv sync

echo "ðŸ§ª Testing system setup..."
uv run scripts/test_setup.py

echo "âœ… Setup complete!"
echo ""
echo "ðŸ“‹ Next steps:"
echo "1. Add your model files (.gguf or .bin) to the models/ directory"
echo "2. Run: uv run main.py"
echo ""
echo "ðŸ“– For detailed instructions, see README.md"
