#!/bin/bash

# LLM Benchmarking System Setup Script

set -e

echo "ðŸš€ Setting up LLM Benchmarking System..."

# Check if Docker is installed
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker is not installed. Please install Docker first."
    exit 1
fi

# Check if Docker Compose is available (modern docker compose)
if ! docker compose version &> /dev/null; then
    echo "âŒ Docker Compose is not available. Please install Docker Compose plugin."
    echo "   On Ubuntu/Debian: sudo apt-get install docker-compose-plugin"
    echo "   On other systems: Follow Docker documentation"
    exit 1
fi

# Check NVIDIA Docker support for CUDA
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… NVIDIA GPU detected"
    if docker info | grep -q nvidia; then
        echo "âœ… NVIDIA Docker runtime available"
    else
        echo "âš ï¸  NVIDIA Docker runtime not found. CUDA benchmarking may not work."
    fi
else
    echo "âš ï¸  NVIDIA GPU not detected. CUDA benchmarking will be skipped."
fi

# Pull prebuilt Docker images
echo "ðŸ³ Pulling prebuilt Docker images..."

# Pull CPU image (full version includes llama-bench)
echo "  Pulling CPU image..."
docker pull ghcr.io/ggml-org/llama.cpp:full

# Pull CUDA image if nvidia-smi is available (full-cuda version includes llama-bench)
if command -v nvidia-smi &> /dev/null; then
    echo "  Pulling CUDA image..."
    docker pull ghcr.io/ggml-org/llama.cpp:full-cuda
else
    echo "  Skipping CUDA image (no NVIDIA GPU detected)"
fi

echo "ðŸ“¦ Installing Uv dependencies..."
uv sync

echo "ðŸ§ª Testing system setup..."
uv run scripts/test_setup.py

echo "âœ… Setup complete!"
echo ""
echo "ðŸ“‹ Next steps:"
echo "1. Add your model files (.gguf or .bin) to the models/ directory"
echo "2. Run: uv run main.py"
echo ""
echo "ðŸ“– For detailed instructions, see README.md"
