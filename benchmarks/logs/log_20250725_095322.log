Verifying Docker images...
✅ Found prebuilt image: ghcr.io/ggml-org/llama.cpp:full
✅ Found prebuilt image: ghcr.io/ggml-org/llama.cpp:full-cuda
✅ All required Docker images are available

📊 Starting benchmark run: 20250725_095323

📊 Benchmarking model: DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf
  🔍 Running cpu variant...
Running benchmark: DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf - cpu
🐳 Using prebuilt image: ghcr.io/ggml-org/llama.cpp:full
📂 Models directory (absolute): /home/andrepascoa/projects/llmtests/benchmarks/models
🐳 Starting container with command: --bench -m /models/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf -o jsonl -fa 1 -b 1,2,4,8,16,32,128,512,1024,2048,5012 -n 0 -p 0 -pg 128,128 -pg 128,512 -pg 512,128 -pg 512,512 -ngl 0 -nopo 1
📊 Container output (streaming to file):
==================================================
load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
{"build_commit": "a12363bb", "build_number": 5974, "cpu_info": "Intel(R) Core(TM) i7-10870H CPU @ 2.20GHz", "gpu_info": "", "backends": "CPU", "model_filename": "/models/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf", "model_type": "qwen2 32B Q4_K - Medium", "model_size": 19845357568, "model_n_params": 32763876352, "n_batch": 1, "n_ubatch": 512, "n_threads": 8, "cpu_mask": "0x0", "cpu_strict": false, "poll": 50, "type_k": "f16", "type_v": "f16", "n_gpu_layers": 0, "split_mode": "layer", "main_gpu": 0, "no_kv_offload": false, "flash_attn": true, "tensor_split": "0.00", "tensor_buft_overrides": "none", "defrag_thold": -1.000000, "use_mmap": true, "embeddings": false, "no_op_offload": 1, "n_prompt": 128, "n_gen": 128, "n_depth": 0, "test_time": "2025-07-25T08:53:41Z", "avg_ns": 154946153085, "stddev_ns": 31916732, "avg_ts": 1.652187, "stddev_ts": 0.000340, "samples_ns": [ 154898659348, 154974594120, 154974000932, 154931856423, 154951654602 ],"samples_ts": [ 1.65269, 1.65188, 1.65189, 1.65234, 1.65213 ]}
