Verifying Docker images...
âœ… Found prebuilt image: ghcr.io/ggml-org/llama.cpp:full
âœ… Found prebuilt image: ghcr.io/ggml-org/llama.cpp:full-cuda
âœ… All required Docker images are available

ğŸ“Š Starting benchmark run: 20250726_165026

ğŸ“Š Benchmarking model: DeepSeek-R1-Distill-Qwen-1.5B-BF16.gguf

SCENARIO 2: CPU scenarios with increasing CPU core count
--------------------------------------------------
Running benchmark: DeepSeek-R1-Distill-Qwen-1.5B-BF16.gguf - cpu - CPU 1 cores - GPU 0%
ğŸ³ Using prebuilt image: ghcr.io/ggml-org/llama.cpp:full
ğŸ“‚ Models directory (absolute): /home/andrepascoa/projects/llmtests/benchmarks/models
ğŸ³ Starting container with command: --bench -m /models/DeepSeek-R1-Distill-Qwen-1.5B-BF16.gguf -o jsonl -fa 1 -b 1,2,4,8,16,32 -n 0 -p 0 -pg 128,128 -pg 128,512 -pg 512,128 -pg 512,512 -mmp 0 -nopo 1 -t 1 -ngl 0
ğŸ“Š Container output (streaming to file):
==================================================
load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
