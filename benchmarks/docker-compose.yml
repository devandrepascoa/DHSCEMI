version: '3.8'

services:
  llama-bench-cuda:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    volumes:
      - ./models:/models:ro
      - ./results:/results:rw
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: tail -f /dev/null

  llama-bench-cpu:
    image: ghcr.io/ggml-org/llama.cpp:full
    volumes:
      - ./models:/models:ro
      - ./results:/results:rw
    command: tail -f /dev/null

  benchmark-runner:
    image: python:3.9-slim
    volumes:
      - ./:/workspace
      - ./models:/models:ro
      - ./results:/results:rw
    working_dir: /workspace
    command: python scripts/benchmark.py
    depends_on:
      - llama-bench-cuda
      - llama-bench-cpu
